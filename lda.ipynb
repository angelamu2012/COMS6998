{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion:  trust\n",
      "['trust', 'covid', 'help', 'vaccine', 'know', 'information', 'vaccines', 'accept', 'pandemic', 'able', 'others', 'vaccinated', 'better', 'virus', 'good', 'risk', 'catching', 'need', 'opinions', 'knowing']\n",
      "emotion:  disgust\n",
      "['disgusted', 'covid', 'pandemic', 'vaccine', 'vaccinated', 'mask', 'disgusting', 'vaccines', 'enough', 'time', 'vaccination', 'disgusts', 'care', 'need', 'taking', 'around', 'virus', 'family', 'tested', 'health']\n",
      "emotion:  joy\n",
      "['happy', 'covid', 'vaccinated', 'pandemic', 'vaccine', 'good', 'positive', 'time', 'news', 'able', 'life', 'back', 'better', 'virus', 'normal', 'everyone', 'vaccines', 'feeling', 'shot', 'much']\n",
      "emotion:  confusion\n",
      "['confused', 'whether', 'covid', 'symptoms', 'vaccine', 'safe', 'positive', 'booster', 'risk', 'vaccinated', 'negative', 'tested', 'omicron', 'family', 'need', 'tests', 'exposed', 'stay', 'long', 'quarantine']\n",
      "emotion:  resilience\n",
      "['pandemic', 'hope', 'good', 'positive', 'news', 'year', 'vaccines', 'covid', 'optimistic', 'hard', 'last', 'better', 'share', 'less', 'symptoms', 'vaccinated', 'omicron', 'forever', 'lose', 'thankful']\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    domain_stop_words = {'someone', 'work', 'things', 'like', 'would', 'going', 'feel', 'still', 'little', 'test', 'want', 'since', 'really', 'also', 'even', 'could', 'getting', 'make', 'though', 'people'}\n",
    "    cleaned = []\n",
    "    for document in corpus:\n",
    "        words = nltk.word_tokenize(document) #tokenize the document content\n",
    "        words = [w.lower() for w in words if w.lower() not in domain_stop_words and w.lower() not in stopwords.words('english') and w.isalpha() and len(w) > 3]\n",
    "        cleaned.append(words)\n",
    "    vocab = []\n",
    "    for text in cleaned:\n",
    "      for word in text:\n",
    "        vocab.append(word)\n",
    "    return cleaned,vocab\n",
    "\n",
    "\n",
    "# filepath = '/Users/angela/Documents/NLPResults/Datasets/BT_only_new.json'\n",
    "# filepath = '/Users/angela/Documents/NLPResults/Datasets/chatgpt_only copy.json'\n",
    "# filepath = '/Users/angela/Documents/NLPResults/Datasets/scraped_only.json'\n",
    "filepath = 'data_withposts/train_val_test_anonymized-WITH_POSTS/train_anonymized-WITH_POSTS_additional_emotions.json'\n",
    "with open(filepath) as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "emotionList = [\"trust\", \"disgust\", \"joy\", \"confusion\", \"resilience\"]\n",
    "for emotion in emotionList:\n",
    "    documents = []\n",
    "    print(\"emotion: \", emotion)\n",
    "    for k in dataset:\n",
    "        summary = None\n",
    "        emo = 0\n",
    "        for annotation in dataset[k]['Annotations']:\n",
    "            for hit in dataset[k]['Annotations'][annotation]:\n",
    "                if hit['Emotion'] == emotion:\n",
    "                    emo = 1\n",
    "                    summary = hit['Abstractive']\n",
    "        if emo == 1:\n",
    "            # print(summary)\n",
    "            # documents.append(dataset[k]['Reddit Post'][0]) \n",
    "            documents.append(summary) \n",
    "\n",
    "    clean_corp,vocab = clean_corpus(documents)\n",
    "    unique_vocab_dict = Dictionary(clean_corp)\n",
    "    topic_corpus = [unique_vocab_dict.doc2bow(doc) for doc in clean_corp]\n",
    "    lda_model = LdaModel(topic_corpus,id2word=unique_vocab_dict,num_topics=1,passes=10) #build the lda model\n",
    "\n",
    "    topics = lda_model.print_topics(num_topics=1, num_words=20)\n",
    "    for topic in topics:\n",
    "        words = [word.split('\"')[1] for word in topic[1].split() if '\"' in word]\n",
    "        print(words)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

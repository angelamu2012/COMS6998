{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELF-BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/angela/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0008107826373244391"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "#START: COPIED FROM https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "def get_bleu_score(sentence, remaining_sentences):\n",
    "    lst = []\n",
    "    for i in remaining_sentences:\n",
    "        bleu = sentence_bleu([sentence.split()], i.split())\n",
    "        lst.append(bleu)\n",
    "    return lst\n",
    "\n",
    "def calculate_selfBleu(sentences):\n",
    "    '''\n",
    "    sentences - list of sentences generated by NLG system\n",
    "    '''\n",
    "    bleu_scores = []\n",
    "\t\n",
    "    for i in sentences:\n",
    "        sentences_copy = copy.deepcopy(sentences)\n",
    "        remaining_sentences = sentences_copy.remove(i)\n",
    "        bleu = get_bleu_score(i,sentences_copy)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "#END: COPIED FROM https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "sentences = []\n",
    "with open(\"/Users/angela/Documents/NLPResults/Datasets/BT_only.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "emotion = \"trust\"\n",
    "for k in dataset:\n",
    "    emo = 0\n",
    "    for annotation in dataset[k]['Annotations']:\n",
    "        for hit in dataset[k]['Annotations'][annotation]:\n",
    "            if hit['Emotion'] == emotion:\n",
    "                emo = 1\n",
    "    if emo == 1:\n",
    "        sentences.append(dataset[k]['Reddit Post']) #retrieve backtranslated reddit post\n",
    "\n",
    "print(sentences)\n",
    "calculate_selfBleu(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU SCORE - Compare Backtranslation w/ Original Post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/angela/Documents/BT_only_new.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "bleu_scores = []\n",
    "emotion = \"resilience\"\n",
    "c = 0\n",
    "for k in dataset:\n",
    "    emo = 0\n",
    "    original = None\n",
    "    for annotation in dataset[k]['Annotations']:\n",
    "        for hit in dataset[k]['Annotations'][annotation]:\n",
    "            original = hit[\"Reddit Post Original\"]\n",
    "            if hit['Emotion'] == emotion:\n",
    "                emo = 1\n",
    "    if emo == 1:\n",
    "        bleu = sentence_bleu([original.split()], dataset[k]['Reddit Post'][0].split())\n",
    "        bleu_scores.append(bleu)\n",
    "print(np.average(bleu_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lmppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmppl\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('/Users/angela/Documents/NLPResults/Backtranslation/Disgust/summaries_output_for_bertcombined_backtranslation.txt_disgust.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    our_summaries = json.loads(lines[-4]) #retrieve list of generated summaries\n",
    "print(our_summaries)\n",
    "\n",
    "#START: COPIED FROM https://github.com/asahi417/lmppl\n",
    "scorer = lmppl.LM('gpt2')\n",
    "ppl = scorer.get_perplexity(our_summaries)\n",
    "#END: COPIED FROM https://github.com/asahi417/lmppl\n",
    "\n",
    "print(list(zip(our_summaries, ppl)))\n",
    "print(np.average(ppl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SummaCConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SummmaCConv scores for target, annotated summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from summac.model_summac import SummaCConv\n",
    "import json\n",
    "\n",
    "original_docs = []\n",
    "summaries = []\n",
    "emotion = \"trust\"\n",
    "\n",
    "with open(original_docs_fname, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "    summary = None\n",
    "    for k in dataset:\n",
    "        emo = 0\n",
    "        for annotation in dataset[k]['Annotations']:\n",
    "            for hit in dataset[k]['Annotations'][annotation]:\n",
    "                if hit['Emotion'] == emotion:\n",
    "                    emo = 1\n",
    "                    summary = hit['Abstractive']\n",
    "        if emo == 1:\n",
    "            original_docs.append(dataset[k]['Reddit Post']) #retrieve original reddit post\n",
    "            summaries.append(summary) #retrieve target, annotated summaries\n",
    "\n",
    "scores = []\n",
    "\n",
    "#START: COPIED FROM https://github.com/tingofurro/summac\n",
    "model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cpu\", start_file=\"default\", agg=\"mean\")\n",
    "for i, item in enumerate(original_docs):\n",
    "    score_conv1 = model_conv.score([original_docs[i]], [summaries[i]])\n",
    "    #END: COPIED FROM https://github.com/tingofurro/summac\n",
    "    scores.append(score_conv1[\"scores\"][0])\n",
    "    print(\"SummacConv score: %.3f\" % (score_conv1[\"scores\"][0]))\n",
    "\n",
    "print(numpy.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SummmaCConv scores for generated summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "from summac.model_summac import SummaCConv\n",
    "import json\n",
    "\n",
    "original_docs = []\n",
    "generated_summaries = []\n",
    "emotion = \"trust\"\n",
    "\n",
    "with open(summaries_fname) as f:\n",
    "   lines = f.readlines()\n",
    "    our_summaries = json.loads(lines[-4])\n",
    "    for summ in our_summaries:\n",
    "        generated_summaries.append(summ) #retrieve generated summaries \n",
    "\n",
    "with open(original_docs_fname, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "    summary = None\n",
    "    for k in dataset:\n",
    "        emo = 0\n",
    "        for annotation in dataset[k]['Annotations']:\n",
    "            for hit in dataset[k]['Annotations'][annotation]:\n",
    "                if hit['Emotion'] == emotion:\n",
    "                    emo = 1\n",
    "                    summary = hit['Abstractive']\n",
    "        if emo == 1:\n",
    "            original_docs.append(dataset[k]['Reddit Post']) #retrieve original reddit post\n",
    "\n",
    "scores = []\n",
    "\n",
    "#START: COPIED FROM https://github.com/tingofurro/summac\n",
    "model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cpu\", start_file=\"default\", agg=\"mean\")\n",
    "for i, item in enumerate(original_docs):\n",
    "    score_conv1 = model_conv.score([original_docs[i]], [generated_summaries[i]])\n",
    "    #END: COPIED FROM https://github.com/tingofurro/summac\n",
    "\n",
    "    scores.append(score_conv1[\"scores\"][0])\n",
    "    print(\"SummacConv score: %.3f\" % (score_conv1[\"scores\"][0]))\n",
    "\n",
    "print(numpy.average(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as bert_scr\n",
    "import json \n",
    "\n",
    "f1_scores = []\n",
    "i = 0\n",
    "with open('/Users/angela/Documents/summaries_output_for_bertall_.txt_disgust.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, len(lines), 2):\n",
    "        our_summaries = json.loads(lines[i]) #retrieve generated summaries\n",
    "        target_summaries = json.loads(lines[i+1]) #retrieve target summaries \n",
    "        \n",
    "        #START: COPIED FROM https://github.com/honglizhan/CovidET/tree/main\n",
    "        _, _, F1 = bert_scr(our_summaries,\n",
    "                        target_summaries,\n",
    "                        model_type='microsoft/deberta-xlarge-mnli',\n",
    "                        lang='en',\n",
    "                        verbose=True)\n",
    "        f1_scores.append(float(F1.mean()))\n",
    "        #END: COPIED FROM https://github.com/honglizhan/CovidET/tree/main\n",
    "\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/angela/Documents/all_.txt_disgust.json'\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file) \n",
    "    for i, item in enumerate(data):\n",
    "        data[item][3] = f1_scores[i] #load output file generated by detection_summarization.py, replace placeholder value w/ calculated BERTScore\n",
    "with open(path, 'a') as file:\n",
    "    json.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
